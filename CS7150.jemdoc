# jemdoc: menu{MENU}{CS7150.html}, nofooter
= Notheastern CS7150 Deep Learning


== Course Information
Deep learning is a subfield of machine learning that builds highly parameterized (deep) models from large volume of data, widely deployed in many industrial systems.
Applications cover computer vision, language, speech, robotics, etc. Deep learning advances so rapidly that it has become one of the most popular field to study.

This course will introduce students to

1) basic building blocks of deep models, e.g., convolution, activation functions, attention

2) typical architectures, e.g., encoder-decoder for seq-to-seq tasks

3) training and evaluation of these models, e.g., variants of stochastic optimizers

Some cutting edge research topics will also be discussed, e.g., interpretability, model compression, self-supervised pretraining. There will be a course project that encourages student to explore more of these open-ended problems.


== Class \& Links
Lecture: Saturday 9:00 am - 12:20 pm, San Jose Campus 1010/1011

Office hours: Saturday 1:00 pm - 3:00 pm, San Jose Campus 1010/1011

TA: Pratyaksh Bhalla (bhalla.pr@northeastern.edu)

TA office hours: Friday morning (check with TA though)

Textbooks:
- [https://www.deeplearningbook.org Deep Learning] by Goodfellow, Bengio and Courville
- [https://www.bishopbook.com Deep Learning, Foundations and Concepts] by C. M. Bishop and H. Bishop
- [https://arxiv.org/abs/2201.02135 Deep Reinforcement Learning] by Plaat
#- [https://inst.eecs.berkeley.edu/~cs188/sp20/assets/files/SuttonBartoIPRLBook2ndEd.pdf Reinforcement Learning: An Introduction 2nd Edition] by Sutton and Barto

Grade: 20\% homework \+ 20\% paper presentation \+ 20\% midterm exam \+ 40\% project

Homework: Due Friday 11:59pm

Paper presentation: Each student presents one paper in one of the classes. We reward good presenters and actively involved audience.
The paper could be one of those suggested reading materials in the previous classes, or any recent paper identified as relevant by the presenter.
There is no restriction/preference on form of the presentation. Slides may be used for clarity. Whiteboard can be used for derivations if necessary.

Khoury cloud: [files/teaching/Khoury_cloud.pdf instruction1], [files/teaching/Khoury_cloud2.pdf instruction2]

General Policies:
- Attend regularly: If you cannot make it, please email TA. Note: 3+ absence without extenuating reason (e.g., illness, attending conferences) can harm your grade
- Academic integrity (check [https://osccr.sites.northeastern.edu/academic-integrity-policy/ here]): In your homework or final project,
	-- if you copied code somewhere from the web, please put a comment of the link
	-- if you discussed with others, please acknowledge your collaborator
	-- if you used chatgpt or any of its equivalence, please give details on how you prompted the language model.

== Schedules
01\/13
- Topics: Introduction, Prerequisites [files/teaching/deep_learning/1.pdf \[slides\]] [https://northeastern-my.sharepoint.com/:v:/g/personal/jia_huang_northeastern_edu/EXDfxc6KbTJHkcUIA23H2c4BDoRfaeM0VRCCLiWOvLcwVw?referrer=Teams.TEAMS-ELECTRON&referrerScenario=MeetingChicletGetLink.view.view \[recording 1], [https://northeastern-my.sharepoint.com/:v:/g/personal/jia_huang_northeastern_edu/ET0nHmUWNfRKmhQt6JdLZzMBjVZlWzT1Zflca7Q-7C_yYg?referrer=Teams.TEAMS-ELECTRON&referrerScenario=MeetingChicletGetLink.view.view 2\]]
- Readings: textbook chapters [https://www.deeplearningbook.org/contents/linear_algebra.html linear algerbra], [https://www.deeplearningbook.org/contents/prob.html probabilities], [https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html PyTorch tutorial page]
- Assignment: [files/teaching/deep_learning/hw1.pdf \[homework1\]]

01\/20
- Topics: Machine Learning Review, Classifiers, Multi-layer perceptron [files/teaching/deep_learning/2.pdf \[slides\]]
- Readings: textbook chapter [https://www.deeplearningbook.org/contents/ml.html ML basics], [https://aclanthology.org/P19-1399.pdf hubless NN], [https://arxiv.org/pdf/2010.15327.pdf deep vs shallow representations], [https://arxiv.org/pdf/1511.07289.pdf ELU activation], [https://arxiv.org/pdf/1606.08415.pdf GELU activation]
- Assignment: [files/teaching/deep_learning/hw2.pdf \[homework2\]]

01\/27
- Topics: Convolutional network and computer vision
- Readings: [https://arxiv.org/pdf/1512.03385.pdf ResNet], [https://arxiv.org/pdf/1608.06993.pdf DenseNet]
- Assignment:

02\/03
- Topics: RNN, LSTM, Transformer
- Readings: [https://arxiv.org/abs/2006.12070 Lipschitz RNN]
- Assignment:

02\/10
- Topics: NLP and Speech applications, Seq-to-Seq Models
- Readings:
- Assignment:

02\/17
- Topics: More on Optimization, Overfitting, Generalization, parallelism
- Readings:
- Assignment:

02\/24
Midterm Exam

03\/02
- Topics: Large Language Models, finetuning methods
- Readings:
- Assignment:

03\/09
- Topics: Sparsity, Distillation, Model Compression
- Readings:
- Assignment:

03\/16:
- Topics: Interpretability, Information bottleneck, adverserial attacks
- Readings:
- Assignment

03\/23
- Topics: Multi-task Learning, meta-learning, zero-shot Learning
- Readings:
- Assignment

03\/30
- Topics: Generative Models
- Readings:
- Assignment

04\/06
- Topics: Reinforcement Learning (Guest Lecture by [https://kaixianglin.github.io Kaixiang Lin])
- Readings:
- Assignment:

04\/13
- Topics: Graph Neural Networks (Guest Lecture by [https://www.linkedin.com/in/jian-zhang-a080271b/ Jian Zhang])
- Readings:
- Assignment:

04\/20
- Final Project Presentation 1

04\/27
- Final Project Presentation 2
